---
title: "Stats 0A"
output: github_document
parent: ta
nav_order: 2
---

---
layout: default
title: Stats 0A
parent: Teaching Assistant
nav_order: 2
---

# Chapter 20. Case Studies

This final chapter applies the statistical tools from earlier chapters—hypothesis testing, means comparisons, regression, ANOVA, transformations, chi-square tests, and nonparametric tests—to real-world examples. Each example loads data directly from URL or built-in packages.

---

# *20.1 Auto MPG*  

Dataset: **Auto MPG** (UCI Machine Learning Repository)  
URL: https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data

---

# 20.1.1 Load, Clean, and Prepare the Data

```{r}
library(tidyverse)

url  <- "https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data"
cols <- c("mpg","cyl","disp","hp","weight","accel","yr","origin","car")

df <- read.table(url, col.names=cols, stringsAsFactors=FALSE)
df <- df[df$hp != "?", ]
df$hp <- as.numeric(df$hp)

summary(df)
```

***

# 20.1.2 Exploratory Visualizations

## Distribution of MPG

```{r}
hist(df$mpg, breaks=20, col="skyblue", main="Distribution of MPG")
```

## Pairwise Scatterplot Matrix

```{r}
pairs(df[,c("mpg","weight","hp","disp")], col="darkgreen")
```

## Correlation Heatmap

```{r}
library(corrplot)
corrplot(cor(df[,c("mpg","cyl","disp","hp","weight","accel")]),
         method="color")
```

***

# 20.1.3 Classical Statistical Analyses

## Simple Linear Regression: mpg \~ weight

```{r}
m1 <- lm(mpg ~ weight, data=df)
summary(m1)
```

### Visual

```{r}
plot(df$weight, df$mpg, pch=19, col="blue")
abline(m1, col="red", lwd=3)
```

***

## Multiple Regression

```{r}
m2 <- lm(mpg ~ weight + hp + cyl + disp, data=df)
summary(m2)
```

***

## Polynomial Regression

```{r}
m3 <- lm(mpg ~ poly(weight,2), data=df)
summary(m3)
```

***

## ANOVA: mpg by cylinders

```{r}
a1 <- aov(mpg ~ as.factor(cyl), data=df)
summary(a1)
boxplot(mpg ~ as.factor(cyl), data=df, main="MPG by Cylinders")
```

***

## Nonparametric Alternative (Kruskal–Wallis)

```{r}
kruskal.test(mpg ~ as.factor(cyl), data=df)
```

***

## Effect Size (Eta-Squared)

```{r}
ssb <- sum(tapply(df$mpg, df$cyl,
                  function(x) length(x)*(mean(x)-mean(df$mpg))^2))
sst <- sum((df$mpg - mean(df$mpg))^2)
eta2 <- ssb / sst
eta2
```

***

# 20.1.4 Diagnostics

## Residuals

```{r}
par(mfrow=c(1,2))
plot(fitted(m2), resid(m2), pch=19, main="Residuals vs Fitted")
qqnorm(resid(m2)); qqline(resid(m2))
par(mfrow=c(1,1))
```

## Influence (Cook’s Distance)

```{r}
plot(cooks.distance(m2), type="h", col="red")
```

***

# 20.1.5 PCA and Clustering

## PCA

```{r}
df_num <- df[,c("mpg","cyl","disp","hp","weight","accel")]
pca <- prcomp(df_num, scale=TRUE)
summary(pca)
plot(pca, main="PCA Scree Plot")
biplot(pca)
```

***

## k-Means Clustering

```{r}
set.seed(1)
k3 <- kmeans(df_num, centers=3)
plot(df$weight, df$mpg,
     col=k3$cluster, pch=19,
     main="k-Means Clustering (k=3)")
```

***

# 20.1.6 Exercises (with Insightful Answers)

### **Exercise 1:**

Why is weight such a strong predictor of MPG?

**Answer:** Weight increases engine load, reducing fuel efficiency. Heavier cars require more energy to move.

***

### **Exercise 2:**

Fit a model using only horsepower and plot the results.

```{r}
m_hp <- lm(mpg ~ hp, data=df)
summary(m_hp)
plot(df$hp, df$mpg, pch=19)
abline(m_hp, col="red")
```

***

### **Exercise 3:**

Which variable correlates most strongly (in magnitude) with mpg?

**Answer:**  
Typically, **weight** or **displacement** shows the strongest negative correlation.

***

### **Exercise 4:**

Perform a Kruskal–Wallis test on horsepower by cylinders.

```{r}
kruskal.test(hp ~ as.factor(cyl), data=df)
```

***

### **Exercise 5:**

Fit a polynomial regression of degree 3 and compare AIC.

```{r}
m_poly3 <- lm(mpg ~ poly(weight,3), data=df)
AIC(m1, m3, m_poly3)
```

***

### **Exercise 6:**

Plot residuals of the multiple regression and interpret.

**Insight:**  
Check for heteroscedasticity (cones/funnels) or curvature.

***

### **Exercise 7:**

Run PCA and interpret PC1.

**Answer:**  
PC1 often reflects “vehicle size/power” combining weight, hp, displacement.

***

### **Exercise 8:**

Cluster using k=4 and visualize.

```{r}
k4 <- kmeans(df_num, centers=4)
plot(df$weight, df$mpg, col=k4$cluster, pch=19)
```

***

### **Exercise 9:**

Which regression model (simple or multiple) performs best based on adjusted R²?

**Answer:**  
Multiple regression usually wins because it captures more vehicle attributes.

***

### **Exercise 10:**

Compute Cohen’s f² effect size for the multiple regression:

Formula:

```{r, fig.width=10, fig.height=3, dpi=60}
plot.new()
text(0.5,0.5,
  expression(f^2 == (R^2)/(1-R^2)),
  cex=1.7)
```

```{r}
R2 <- summary(m2)$r.squared
f2 <- R2 / (1 - R2)
f2
```

***

# *20.2 Iris: Multivariate Case Study* 

Dataset: **Iris** (Built-in in R)

The famous Fisher Iris dataset contains 150 flowers, 3 species, and 4 numeric features:
- Sepal.Length  
- Sepal.Width  
- Petal.Length  
- Petal.Width  

This dataset is ideal for classical statistical methods and modern multivariate techniques (PCA, clustering).

---

# 20.2.1 Load and Inspect the Data

```{r}
library(tidyverse)

df <- iris
summary(df)
```

***

# 20.2.2 Exploratory Visualizations

## Histograms

```{r}
par(mfrow=c(2,2))
hist(df$Sepal.Length, main="Sepal Length", col="skyblue")
hist(df$Sepal.Width, main="Sepal Width", col="lightgreen")
hist(df$Petal.Length, main="Petal Length", col="pink")
hist(df$Petal.Width, main="Petal Width", col="khaki")
par(mfrow=c(1,1))
```

## Pairwise Scatterplot Matrix

```{r}
pairs(df[,1:4], col=df$Species, pch=19)
```

## Species Mean Plot

```{r}
df %>% group_by(Species) %>%
  summarize(across(everything(), mean)) %>%
  pivot_longer(-Species) %>%
  ggplot(aes(x=name, y=value, fill=Species)) +
  geom_col(position="dodge") +
  theme_bw() +
  ggtitle("Mean Measurements by Species")
```

***

# 20.2.3 Statistical Analyses

## ANOVA: Sepal.Length \~ Species

```{r}
a1 <- aov(Sepal.Length ~ Species, data=df)
summary(a1)

boxplot(Sepal.Length ~ Species, data=df, col=c("lightblue","pink","lightgreen"))
```

### Nonparametric Alternative

```{r}
kruskal.test(Sepal.Length ~ Species, data=df)
```

***

## Multiple Regression

Predict Sepal.Length from other variables.

```{r}
m1 <- lm(Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width, data=df)
summary(m1)
```

### Diagnostic Plots

```{r}
par(mfrow=c(1,2))
plot(fitted(m1), resid(m1), pch=19)
qqnorm(resid(m1)); qqline(resid(m1))
par(mfrow=c(1,1))
```

***

## Effect Size (Eta-Squared for ANOVA)

```{r}
ss_total <- sum((df$Sepal.Length - mean(df$Sepal.Length))^2)
ss_between <- sum(tapply(df$Sepal.Length, df$Species,
                 function(x) length(x)*(mean(x)-mean(df$Sepal.Length))^2))
eta2 <- ss_between / ss_total
eta2
```

***

# 20.2.4 PCA and Clustering

## PCA (scaled)

```{r}
pca <- prcomp(df[,1:4], scale=TRUE)
summary(pca)

biplot(pca, col=c("red","blue"))
```

## k-Means Clustering

```{r}
set.seed(2)
k3 <- kmeans(df[,1:4], centers=3)
plot(df$Petal.Length, df$Petal.Width, col=k3$cluster, pch=19,
     main="k-Means (k=3)")
```

***

# 20.2.5 Formula Examples Using Required Method

### Multiple Regression Formula

```{r, fig.width=10, fig.height=3, dpi=60}
plot.new()
text(0.5,0.5, expression(Sepal.Length == b[0] + b[1]*Sepal.Width + b[2]*Petal.Length + b[3]*Petal.Width), cex=1.4)
```

### ANOVA F-Ratio

```{r, fig.width=10, fig.height=3, dpi=60}
plot.new()
text(0.5,0.5, expression(F == MS[b]/MS[w]), cex=1.7)
```

***

# 20.2.6 Exercises (with Insightful Answers)

### **Exercise 1 — Visualize species separation using Petal.Length and Petal.Width.**

```{r}
ggplot(df, aes(Petal.Length, Petal.Width, color=Species)) +
  geom_point() + theme_bw()
```

**Insight:** These two dimensions nearly separate the species perfectly.

***

### **Exercise 2 — Compute the correlation matrix and find the strongest correlation.**

```{r}
cor(df[,1:4])
```

**Insight:** Petal Length and Petal Width are most strongly correlated (\~0.96).

***

### **Exercise 3 — Fit a model predicting Petal.Length.**

```{r}
m2 <- lm(Petal.Length ~ Sepal.Length + Sepal.Width + Petal.Width, data=df)
summary(m2)
```

**Insight:** Petal.Width is the strongest predictor.

***

### **Exercise 4 — Check normality of residuals for ANOVA using QQ plot.**

```{r}
qqnorm(resid(a1)); qqline(resid(a1))
```

**Insight:** ANOVA assumptions hold reasonably well.

***

### **Exercise 5 — Use Tukey HSD for post-hoc comparisons.**

```{r}
TukeyHSD(a1)
```

**Insight:** All species differ significantly in Sepal.Length except setosa–versicolor being borderline.

***

### **Exercise 6 — Calculate effect size (Cohen’s f²) for the multiple regression.**

Formula:

```{r, fig.width=10, fig.height=3, dpi=60}
plot.new()
text(0.5,0.5,expression(f^2 == R^2 / (1 - R^2)), cex=1.6)
```

```{r}
R2 <- summary(m1)$r.squared
f2 <- R2/(1 - R2)
f2
```

***

### **Exercise 7 — Cluster using k=2 and interpret.**

```{r}
k2 <- kmeans(df[,1:4], centers=2)
table(k2$cluster, df$Species)
```

**Insight:** One cluster mostly corresponds to Setosa.

***

### **Exercise 8 — Principal component loadings interpretation.**

```{r}
pca$rotation
```

**Insight:** PC1 measures overall “petal size”.

***

### **Exercise 9 — Fit logistic regression: versicolor vs virginica.**

```{r}
df2 <- iris %>% filter(Species != "setosa")
m_log <- glm(Species ~ Petal.Length + Petal.Width, data=df2, family=binomial)
summary(m_log)
```

**Insight:** Petal.Width is the strongest discriminator.

***

### **Exercise 10 — Visualize PCA clusters colored by true species.**

```{r}
pc <- as.data.frame(pca$x)
pc$Species <- df$Species

ggplot(pc, aes(PC1, PC2, color=Species)) +
  geom_point(size=3) +
  theme_bw() +
  ggtitle("True Species in PCA Space")
```

***

# *20.3 Penguins*

## 20.3 Penguins: Advanced Real-Data Analysis  
Dataset: **Palmer Penguins**  
Source URL:  
https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv

This dataset includes:
- 3 species (Adelie, Chinstrap, Gentoo)  
- 2 islands  
- Sex  
- Culmen (bill) length/width  
- Flipper length  
- Body mass  

We will perform:
- Data cleaning  
- EDA  
- Multivariate visualizations  
- ANOVA  
- Multiple regression  
- Logistic regression  
- PCA  
- Clustering  
- 10 Advanced Exercises with solutions  
- Mathematical formulas with your required R plotting method

---

# 20.3.1 Load and Prepare the Data

```{r}
library(tidyverse)

peng <- read.csv("https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv")

# Clean missing rows
peng2 <- peng %>% 
  select(species, island, sex, bill_length_mm, bill_depth_mm,
         flipper_length_mm, body_mass_g) %>%
  drop_na()

summary(peng2)
```

***

# 20.3.2 Exploratory Visualizations

## Histograms of all numeric variables

```{r}
par(mfrow=c(2,2))
hist(peng2$bill_length_mm, main="Bill Length", col="skyblue")
hist(peng2$bill_depth_mm, main="Bill Depth", col="pink")
hist(peng2$flipper_length_mm, main="Flipper Length", col="lightgreen")
hist(peng2$body_mass_g, main="Body Mass", col="khaki")
par(mfrow=c(1,1))
```

## Pairwise Scatterplot Matrix

```{r}
peng2$species <- factor(peng2$species)
pairs(peng2[,4:7], col = peng2$species, pch = 19)
legend(
  "bottomright",
  legend = levels(peng2$species),
  col = seq_along(levels(peng2$species)),
  pch = 19, cex = 0.5
)
```

## Boxplots by Species

```{r}
peng2 %>%
  pivot_longer(cols=bill_length_mm:body_mass_g) %>%
  ggplot(aes(species, value, fill=species)) +
  geom_boxplot() +
  facet_wrap(~name, scales="free") +
  theme_bw()
```

***

# 20.3.3 Statistical Analyses

## ANOVA: Body Mass by Species

```{r}
a1 <- aov(body_mass_g ~ species, data=peng2)
summary(a1)
boxplot(body_mass_g ~ species, data=peng2, col=c("lightblue","pink","lightgreen"))
```

### Nonparametric alternative

```{r}
kruskal.test(body_mass_g ~ species, data=peng2)
```

***

## Multiple Regression

Predict body mass from culmen + flipper lengths:

```{r}
m1 <- lm(body_mass_g ~ bill_length_mm + bill_depth_mm +
           flipper_length_mm, data=peng2)
summary(m1)
```

### Diagnostics

```{r}
par(mfrow=c(1,2))
plot(fitted(m1), resid(m1), pch=19)
qqnorm(resid(m1)); qqline(resid(m1))
par(mfrow=c(1,1))
```

***

## Logistic Regression

Predict sex (male vs female) from body size variables.

```{r}
peng3 <- peng2 %>% filter(sex %in% c("male","female"))
peng3$sex <- factor(peng3$sex)

m_log <- glm(sex ~ bill_length_mm + bill_depth_mm + flipper_length_mm,
             data=peng3, family=binomial)
summary(m_log)
```

***

# 20.3.4 Multivariate Analyses (PCA + Clustering)

## PCA

```{r}
num_vars <- peng2[,c("bill_length_mm","bill_depth_mm",
                     "flipper_length_mm","body_mass_g")]
pca <- prcomp(num_vars, scale=TRUE)

summary(pca)
biplot(pca, col=c("red","blue"))
```

## k-Means Clustering

```{r}
set.seed(3)
k3 <- kmeans(num_vars, centers=3)
plot(peng2$flipper_length_mm, peng2$body_mass_g, 
     col=k3$cluster, pch=19,
     main="k-Means Clustering (k=3)")
```

***

# 20.3.5 Formula Block Examples (Required Style)

### Logistic Regression Formula

```{r, fig.width=10, fig.height=3, dpi=60}
plot.new()
text(0.5,0.5,
     expression(logit(p) == b[0] + b[1]*x[1] + b[2]*x[2] + b[3]*x[3]),
     cex=1.6)
```

### ANOVA F-Ratio

```{r, fig.width=10, fig.height=3, dpi=60}
plot.new()
text(0.5,0.5, expression(F == MS[b] / MS[w]), cex=1.7)
```

***

# 20.3.6 Exercises (With Detailed Insights)

### **Exercise 1 — Compare bill length between species using ANOVA.**

```{r}
anova_bill <- aov(bill_length_mm ~ species, data=peng2)
summary(anova_bill)
```

**Insight:** Gentoo penguins typically show much longer bills.

***

### **Exercise 2 — Perform Tukey post-hoc comparisons.**

```{r}
TukeyHSD(anova_bill)
```

**Insight:** Species differ strongly in bill morphology.

***

### **Exercise 3 — Correlation matrix of numeric variables.**

```{r}
cor(num_vars)
```

**Insight:** Flipper length and body mass show strong positive correlation.

***

### **Exercise 4 — Fit a model predicting flipper length.**

```{r}
m_flip <- lm(flipper_length_mm ~ bill_length_mm + body_mass_g, data=peng2)
summary(m_flip)
```

**Insight:** Body mass is a strong predictor.

***

### **Exercise 5 — Compare body mass across islands using Kruskal–Wallis.**

```{r}
kruskal.test(body_mass_g ~ island, data=peng2)
```

**Insight:** Island populations differ significantly.

***

### **Exercise 6 — Create a scatterplot colored by island.**

```{r}
ggplot(peng2, aes(bill_length_mm, bill_depth_mm, color=island)) +
  geom_point() + theme_bw()
```

**Insight:** Islands have different morphotype distributions.

***

### **Exercise 7 — Identify influential points in regression m1.**

```{r}
plot(cooks.distance(m1), type="h")
```

**Insight:** A few unusually large birds act as high-leverage points.

***

### **Exercise 8 — Cluster with k=4 and evaluate species alignment.**

```{r}
k4 <- kmeans(num_vars, centers=4)
table(k4$cluster, peng2$species)
```

**Insight:** Clusters partially overlap species but reveal substructure.

***

### **Exercise 9 — PCA visualization colored by sex.**

```{r}
pc <- as.data.frame(pca$x)
pc$sex <- peng2$sex

ggplot(pc, aes(PC1, PC2, color=sex)) +
  geom_point(size=3) + theme_bw()
```

**Insight:** Male and female morphologies show partial separation.

***

### **Exercise 10 — Compute eta² for body mass by species.**

```{r}
ss_t <- sum((peng2$body_mass_g - mean(peng2$body_mass_g))^2)
ss_b <- sum(tapply(peng2$body_mass_g, peng2$species,
                   function(x) length(x)*(mean(x)-mean(peng2$body_mass_g))^2))
eta2 <- ss_b / ss_t
eta2
```

**Insight:** Species explain a large share of variance in body mass.

***

# *20.4 Titanic — Real Data Subchapter*

## 20.4 Titanic: A Comprehensive Real Data Survival Case Study  
Dataset: **Titanic (Kaggle Mirror)**  
Source: https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv

This dataset contains:
- Passenger demographics  
- Class  
- Fare  
- Age  
- Cabin  
- Embarked port  
- Survival outcome  

We will analyze:
- Data cleaning  
- Exploratory visualizations  
- Categorical analysis (chi‑square)  
- Logistic regression  
- Effect sizes  
- PCA  
- Clustering  
- 10 advanced exercises with solutions  

---

# 20.4.1 Load and Prepare the Data

```{r}
library(tidyverse)

url <- "https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv"
titanic <- read.csv(url)

# Basic cleaning
df <- titanic %>%
  select(Survived, Pclass, Sex, Age, Fare, Embarked) %>%
  drop_na()

df$Survived <- factor(df$Survived, labels=c("Died","Survived"))
df$Sex      <- factor(df$Sex)
df$Embarked <- factor(df$Embarked)

summary(df)
```

***

# 20.4.2 Exploratory Visualizations

## Survival counts

```{r}
ggplot(df, aes(Survived, fill=Survived)) +
  geom_bar() + theme_bw()
```

## Age Distribution by Survival

```{r}
ggplot(df, aes(Age, fill=Survived)) +
  geom_histogram(position="identity", alpha=0.5) +
  theme_bw()
```

## Fare vs Age

```{r}
ggplot(df, aes(Age, Fare, color=Survived)) +
  geom_point(alpha=0.7) + theme_bw()
```

## Class and Survival Mosaic

```{r}
library(ggplot2)

ggplot(df, aes(x = Pclass, fill = Survived)) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = scales::percent) +
  labs(y = "Proportion") +
  theme_bw()
```

***

# 20.4.3 Chi‑Square Tests

## Survival vs Sex

```{r}
tbl1 <- table(df$Sex, df$Survived)
chisq.test(tbl1)
tbl1
```

## Survival vs Class

```{r}
tbl2 <- table(df$Pclass, df$Survived)
chisq.test(tbl2)
tbl2
```

### Formula Block (Chi‑Square)

```{r, fig.width=10, fig.height=3, dpi=60}
plot.new()
text(0.5,0.5,expression(chi^2 == sum((O - E)^2 / E)), cex=1.6)
```

***

# 20.4.4 Logistic Regression (Survival Model)

```{r}
log1 <- glm(Survived ~ Sex + Age + Fare + Pclass, 
            data=df, family=binomial)
summary(log1)
```

## Odds Ratios

```{r}
exp(coef(log1))
```

## Prediction Plot

```{r}
df$pred <- predict(log1, type="response")

ggplot(df, aes(pred, fill=Survived)) +
  geom_histogram(alpha=0.5, position="identity") +
  theme_bw()
```

***

# 20.4.5 Diagnostics and Effect Sizes

## ROC Curve

```{r}
library(pROC)
roc1 <- roc(df$Survived, df$pred)
plot(roc1, col="blue", lwd=3)
auc(roc1)
```

## Pseudo-R² (McFadden)

```{r}
1 - logLik(log1)/logLik(glm(Survived ~ 1, data=df, family=binomial))
```

***

# 20.4.6 PCA and Clustering

## Numeric PCA

```{r}
num <- df %>% select(Age, Fare, Pclass)
pca <- prcomp(scale(num))
summary(pca)
biplot(pca)
```

## k-Means Clustering

```{r}
set.seed(10)
k3 <- kmeans(scale(num), centers=3)
plot(df$Age, df$Fare, col=k3$cluster, pch=19,
     main="k-Means Clustering (k=3)")
```

***

# 20.4.7 Required Formula Example (Logistic Regression)

```{r, fig.width=10, fig.height=3, dpi=60}
plot.new()
text(0.5,0.5,
     expression(logit(p) == b[0] + b[1]*Sex + b[2]*Age + b[3]*Pclass + b[4]*Fare),
     cex=1.4)
```

***

# 20.4.8 Exercises (with Detailed Solutions)

### **Exercise 1 — Perform chi‑square test: Embarked vs Survival**

```{r}
tbl3 <- table(df$Embarked, df$Survived)
chisq.test(tbl3)
```

**Insight:** Where passengers boarded strongly influences survival (lifeboat access and crowding).

***

### **Exercise 2 — Compare Ages by Survival using Wilcoxon test**

```{r}
wilcox.test(Age ~ Survived, data=df)
```

**Insight:** Survivors tend to be younger.

***

### **Exercise 3 — Fit a reduced logistic model using only Sex and Class**

```{r}
log2 <- glm(Survived ~ Sex + Pclass, data=df, family=binomial)
summary(log2)
```

**Insight:** Sex alone explains huge variance; Class adds strong stratification.

***

### **Exercise 4 — Compute odds ratio for being female**

```{r}
exp(coef(log2)["Sexfemale"])
```

**Insight:** Females have drastically higher odds of survival.

***

### **Exercise 5 — Run PCA on Age + Fare only**

```{r}
pca2 <- prcomp(df[,c("Age","Fare")], scale=TRUE)
summary(pca2)
```

**Insight:** PC1 captures socioeconomic status (age + fare).

***

### **Exercise 6 — Cluster passengers into 2 groups (k=2)**

```{r}
k2 <- kmeans(scale(num), centers=2)
table(k2$cluster, df$Survived)
```

**Insight:** One cluster disproportionately groups survivors.

***

### **Exercise 7 — Visualize Pclass vs Fare by Survival**

```{r}
ggplot(df, aes(Pclass, Fare, color=Survived)) +
  geom_jitter(width=0.2) + theme_bw()
```

**Insight:** Survivors often come from higher-fare classes.

***

### **Exercise 8 — Fit a model predicting Survival using Fare only**

```{r}
log_f <- glm(Survived ~ Fare, data=df, family=binomial)
summary(log_f)
```

**Insight:** Fare alone has predictive power (proxy for class).

***

### **Exercise 9 — Compute eta‑squared for ANOVA: Fare \~ Survived**

```{r}
a3 <- aov(Fare ~ Survived, data=df)
ss_t <- sum((df$Fare - mean(df$Fare))^2)
ss_b <- sum(tapply(df$Fare, df$Survived,
                   function(x) length(x)*(mean(x)-mean(df$Fare))^2))
eta2 <- ss_b/ss_t
eta2
```

**Insight:** Survival status explains a meaningful fraction of fare variance.

***

### **Exercise 10 — Plot predicted probability vs Age**

```{r}
ggplot(df, aes(Age, pred, color=Survived)) +
  geom_point() + theme_bw()
```

**Insight:** Very young passengers show higher survival probability.

***

# *20.5 GSS*  

## 20.5 GSS: Real-Data Sociodemographic Analysis  
Dataset: **GSS Vocab (General Social Survey Extract)**  
Source URL:  
https://vincentarelbundock.github.io/Rdatasets/csv/carData/GSSvocab.csv

The dataset includes:
- `vocab` — vocabulary test score  
- `educ` — years of education  
- `age`  
- `nativeBorn`  
- `sex`  
- `year` of survey  

This subchapter includes:
- Data cleaning  
- EDA  
- Correlations  
- Regression  
- ANOVA  
- Nonparametric tests  
- Logistic regression (native-born prediction)  
- PCA  
- Clustering  
- 10 advanced exercises with full solutions  

---

# 20.5.1 Load and Prepare the Data

```{r}
library(tidyverse)

url <- "https://vincentarelbundock.github.io/Rdatasets/csv/carData/GSSvocab.csv"
gss <- read.csv(url)

# clean and select core variables
gss2 <- gss %>%
  select(vocab, educ, age, gender, nativeBorn, year) %>%
  drop_na()

summary(gss2)
```

***

# 20.5.2 Exploratory Visualizations

## Histograms

```{r}
par(mfrow=c(1,3))
hist(gss2$vocab, col="skyblue", main="Vocabulary")
hist(gss2$educ,  col="lightgreen", main="Education")
hist(gss2$age,   col="pink", main="Age")
par(mfrow=c(1,1))
```

## Vocabulary vs Education

```{r}
ggplot(gss2, aes(educ, vocab)) +
  geom_point(alpha=0.4) +
  geom_smooth(method="lm") +
  theme_bw()
```

## Distribution by Gender

```{r}
ggplot(gss2, aes(gender, vocab, fill=gender)) +
  geom_boxplot() +
  theme_bw()
```

***

# 20.5.3 Statistical Analyses

## Correlation Matrix

```{r}
cor(gss2[,c("vocab","educ","age")])
```

***

## Simple Linear Regression: vocab \~ educ

```{r}
m1 <- lm(vocab ~ educ, data=gss2)
summary(m1)
```

### Visual

```{r}
plot(gss2$educ, gss2$vocab, pch=19, col="blue")
abline(m1, col="red", lwd=2)
```

***

## Multiple Regression

```{r}
m2 <- lm(vocab ~ educ + age + gender, data=gss2)
summary(m2)
```

***

## ANOVA: vocabulary across gender

```{r}
a1 <- aov(vocab ~ gender, data=gss2)
summary(a1)
boxplot(vocab ~ gender, data=gss2, col=c("lightblue","pink"))
```

***

## Nonparametric Test (Wilcoxon)

```{r}
wilcox.test(vocab ~ gender, data=gss2)
```

***

## Logistic Regression: Predict native-born

```{r}
gss2$nativeBorn <- factor(gss2$nativeBorn)

m_log <- glm(nativeBorn ~ educ + age + vocab, data=gss2, family=binomial)
summary(m_log)
exp(coef(m_log))
```

***

# 20.5.4 PCA and Clustering

## PCA

```{r}
num <- gss2[,c("vocab","educ","age")]
pca <- prcomp(num, scale=TRUE)
summary(pca)
biplot(pca)
```

## k-Means Clustering (k=3)

```{r}
set.seed(5)
k3 <- kmeans(scale(num), centers=3)
plot(gss2$educ, gss2$vocab, col=k3$cluster, pch=19,
     main="k-Means: Education vs Vocab")
```

***

# 20.5.5 Required Formula Blocks

### Multiple Regression Formula

```{r, fig.width=10, fig.height=3, dpi=60}
plot.new()
text(0.5,0.5,
     expression(vocab == b[0] + b[1]*educ + b[2]*age + b[3]*gender),
     cex=1.6)
```

### Logistic Regression Formula

```{r, fig.width=10, fig.height=3, dpi=60}
plot.new()
text(0.5,0.5,
     expression(logit(p(nativeBorn)) == b[0] + b[1]*educ + b[2]*age + b[3]*vocab),
     cex=1.4)
```

***

# 20.5.6 Exercises (with Detailed Solutions)

### **Exercise 1 — Create a scatterplot matrix of all numeric variables.**

```{r}
pairs(gss2[,c("vocab","educ","age")], col="darkgreen")
```

**Insight:** Vocabulary clusters strongly with education.

***

### **Exercise 2 — Compute Spearman correlation between vocabulary and age.**

```{r}
cor.test(gss2$vocab, gss2$age, method="spearman")
```

**Insight:** Weak positive correlation.

***

### **Exercise 3 — Fit a second-order polynomial model vocab \~ educ².**

```{r}
m_poly <- lm(vocab ~ poly(educ,2), data=gss2)
summary(m_poly)
```

**Insight:** Slight curvature but linear model is adequate.

***

### **Exercise 4 — Conduct ANOVA by survey year (vocab \~ year).**

```{r}
a_year <- aov(vocab ~ as.factor(year), data=gss2)
summary(a_year)
```

**Insight:** Vocabulary varies across decades.

***

### **Exercise 5 — Compare education between native-born vs non-native using Wilcoxon.**

```{r}
#table(gss2$nativeBorn)
#wilcox.test(educ ~ nativeBorn, data=gss2)

# Keep only valid groups (native-born = yes or no)
gss2_clean <- subset(gss2, nativeBorn %in% c("yes", "no"))

# Wilcoxon rank-sum test comparing education by native-born status
wilcox.test(educ ~ nativeBorn, data = gss2_clean)
```

**Insight:** Differences reflect demographic shifts.

***

### **Exercise 6 — Fit reduced logistic model predicting native-born using only education.**

```{r}
m_log2 <- glm(nativeBorn ~ educ, data=gss2, family=binomial)
summary(m_log2)
exp(coef(m_log2))
```

**Insight:** More education slightly increases likelihood of being native-born.

***

### **Exercise 7 — Cluster using k=4 and cross-tabulate against gender.**

```{r}
k4 <- kmeans(scale(num), centers=4)
table(k4$cluster, gss2$gender)
```

**Insight:** Gender differences appear in some clusters.

***

### **Exercise 8 — PCA: Plot PC1 vs PC2 colored by native-born.**

```{r}
pc <- as.data.frame(pca$x)
pc$nativeBorn <- gss2$nativeBorn

ggplot(pc, aes(PC1, PC2, color=nativeBorn)) +
  geom_point() + theme_bw()
```

**Insight:** Subtle grouping appears along PC1.

***

### **Exercise 9 — Compute effect size (eta²) for ANOVA by gender.**

```{r}
ss_total <- sum((gss2$vocab - mean(gss2$vocab))^2)
ss_between <- sum(tapply(gss2$vocab, gss2$gender,
                   function(x) length(x)*(mean(x)-mean(gss2$vocab))^2))
eta2 <- ss_between/ss_total
eta2
```

**Insight:** Gender explains only a modest share of variance in vocabulary.

***

### **Exercise 10 — Fit a regression predicting education from age & vocabulary.**

```{r}
m_rev <- lm(educ ~ vocab + age, data=gss2)
summary(m_rev)
```

**Insight:** Vocabulary strongly predicts schooling; age adds generational variation.

***

# *20.6 Flights*

## 20.6 Flights: NYC Airline Delays, Transformations, Regression, PCA & Clustering  
Dataset: **nycflights13**  
Source: R package `nycflights13` — 336,776 commercial US flights from 2013 departing NYC (JFK, LGA, EWR).

This case study analyzes:
- Delays  
- Weather interactions  
- Predictive modeling  
- Transformations  
- PCA  
- Clustering  
- Multiple statistical tests  
- 10 applied exercises with solutions  

---

# 20.6.1 Load and Inspect the Data

```{r}
library(tidyverse)
library(nycflights13)

df <- flights %>%
  select(year, month, day, dep_time, dep_delay, arr_delay,
         carrier, flight, origin, dest, air_time, distance) %>%
  drop_na()

summary(df)
```

***

# 20.6.2 Exploratory Visualizations

## Distribution of Departure Delays

```{r}
hist(df$dep_delay, breaks=60, col="skyblue",
     main="Distribution of Departure Delays")
```

## Boxplot by Airport

```{r}
ggplot(df, aes(origin, dep_delay, fill=origin)) +
  geom_boxplot() +
  coord_cartesian(ylim=c(-10,200)) +
  theme_bw()
```

## Monthly Mean Delays

```{r}
df %>%
  group_by(month) %>%
  summarize(mean_delay = mean(dep_delay)) %>%
  ggplot(aes(month, mean_delay)) +
  geom_line(lwd=2, col="red") +
  geom_point(size=3) +
  theme_bw()
```

***

# 20.6.3 Statistical Analyses

## Correlation Among Numeric Variables

```{r}
cor(df %>% select(dep_delay, arr_delay, air_time, distance))
```

***

## Simple Regression: arr\_delay \~ dep\_delay

```{r}
m1 <- lm(arr_delay ~ dep_delay, data=df)
summary(m1)
```

### Visual

```{r}
df_small <- df[sample(nrow(df), 3000), ]
plot(df_small$dep_delay, df_small$arr_delay, pch=19, col="gray")
abline(m1, col="red", lwd=2)
```

***

## Multiple Regression: arr\_delay \~ dep\_delay + distance + air\_time

```{r}
m2 <- lm(arr_delay ~ dep_delay + distance + air_time, data=df)
summary(m2)
```

***

## ANOVA: Delays by Origin Airport

```{r}
a1 <- aov(dep_delay ~ origin, data=df)
summary(a1)

boxplot(dep_delay ~ origin, data=df, col=c("lightblue","pink","lightgreen"))
```

***

## Nonparametric Alternative (Kruskal-Wallis)

```{r}
kruskal.test(dep_delay ~ origin, data=df)
```

***

# 20.6.4 Transformations (Log)

Delays are skewed; apply log transform to positive values.

```{r}
df_pos <- df %>% filter(dep_delay > 0)
par(mfrow=c(1,2))
hist(df_pos$dep_delay, main="Raw Delay", col="skyblue")
hist(log(df_pos$dep_delay), main="Log-Transformed", col="pink")
par(mfrow=c(1,1))
```

***

# 20.6.5 PCA and Clustering

## PCA of delay/flight characteristics

```{r}
num <- df %>% select(dep_delay, arr_delay, air_time, distance) %>% drop_na()

pca <- prcomp(num, scale=TRUE)
summary(pca)
biplot(pca)
```

***

## k-Means Clustering (k=3)

```{r}
set.seed(4)
k3 <- kmeans(scale(num), centers=3)

df_k <- df %>% drop_na(dep_delay, arr_delay, air_time, distance) %>%
  mutate(cluster = factor(k3$cluster))

ggplot(df_k[sample(nrow(df_k), 4000), ],
       aes(dep_delay, arr_delay, color=cluster)) +
  geom_point(alpha=0.6) +
  theme_bw()
```

***

# 20.6.6 Formula Blocks (Required Display Style)

## Regression Equation

```{r, fig.width=10, fig.height=3, dpi=60}
plot.new()
text(0.5,0.5,
     expression(arr_delay == b[0] + b[1]*dep_delay + b[2]*distance + b[3]*air_time),
     cex=1.4)
```

## ANOVA F-Ratio

```{r, fig.width=10, fig.height=3, dpi=60}
plot.new()
text(0.5,0.5, expression(F == MS[b] / MS[w]), cex=1.7)
```

***

# 20.6.7 Exercises (with Insightful Answers)

### **Exercise 1 — Which month has the highest average arrival delay?**

```{r}
df %>% group_by(month) %>% summarize(m=mean(arr_delay)) %>% arrange(desc(m))
```

**Insight:** Summer months show congestion effects.

***

### **Exercise 2 — Compare departure delays across carriers using ANOVA.**

```{r}
a_carrier <- aov(dep_delay ~ carrier, data=df)
summary(a_carrier)
```

**Insight:** Carriers differ significantly in operational performance.

***

### **Exercise 3 — Apply Kruskal-Wallis for robustness.**

```{r}
kruskal.test(dep_delay ~ carrier, data=df)
```

***

### **Exercise 4 — Compute correlation between air\_time and distance.**

```{r}
cor(df$air_time, df$distance)
```

**Insight:** Strong positive correlation reflecting flight physics.

***

### **Exercise 5 — Fit regression arr\_delay \~ dep\_delay + month.**

```{r}
m_month <- lm(arr_delay ~ dep_delay + factor(month), data=df)
summary(m_month)
```

**Insight:** Month effects remain after adjusting for departure delay.

***

### **Exercise 6 — Logistic Regression: On-time (<5 min delay) vs Late.**

```{r}
df2 <- df %>% mutate(on_time = ifelse(arr_delay <= 5, 1, 0))

log1 <- glm(on_time ~ dep_delay + distance + air_time,
            data=df2, family=binomial)
summary(log1)
```

**Insight:** Departure delay dominates on-time probability.

***

### **Exercise 7 — PCA: Interpret PC1.**

```{r}
pca$rotation
```

**Insight:** PC1 represents an overall “delay magnitude” component.

***

### **Exercise 8 — Cluster into k=4 and cross-tabulate with airport.**

```{r}
k4 <- kmeans(scale(num), centers=4)
table(k4$cluster, df$origin)
```

**Insight:** Clusters partially differentiate airports.

***

### **Exercise 9 — Test whether median delays differ by origin (Wilcoxon).**

```{r}
pairwise.wilcox.test(df$dep_delay, df$origin)
```

***

### **Exercise 10 — Compute eta² for ANOVA: dep\_delay \~ origin.**

```{r}
ss_total <- sum((df$dep_delay - mean(df$dep_delay))^2)
ss_between <- sum(tapply(df$dep_delay, df$origin,
                         function(x) length(x)*(mean(x)-mean(df$dep_delay))^2))
eta2 <- ss_between / ss_total
eta2
```

**Insight:** Origin explains a notable fraction of delay variability.

***
